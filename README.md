# ğŸ§  Project Red-RL: Autonomous PokÃ©mon Yellow Agent

![Status](https://img.shields.io/badge/Status-Active_Development-success)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![Framework](https://img.shields.io/badge/RL-Stable--Baselines3-orange)
![Emulator](https://img.shields.io/badge/Emulator-PyBoy-purple)

> **Arquitectura:** PPO (Proximal Policy Optimization) + DecodificaciÃ³n de Estado Neuro-SimbÃ³lica.
> **Objetivo:** Entrenar un agente de Inteligencia Artificial capaz de completar *PokÃ©mon EdiciÃ³n Amarilla* desde cero, sin conocimiento previo (Tabula Rasa).

## ğŸ“‹ DescripciÃ³n TÃ©cnica

Este proyecto implementa una arquitectura de **Aprendizaje por Refuerzo Profundo (Deep RL)** diseÃ±ada para resolver entornos de RPG complejos con un horizonte temporal extremadamente largo. 

A diferencia de los enfoques puramente visuales (que solo "ven" pÃ­xeles), este sistema utiliza un **Espacio de ObservaciÃ³n HÃ­brido** que combina:
1.  **VisiÃ³n (CNN):** Procesamiento de la pantalla para entender la geometrÃ­a local y obstÃ¡culos.
2.  **Memoria (RAM):** Lectura directa de la memoria del sistema emulado para obtener contexto global (coordenadas, mapa ID, medallas).

### âœ¨ CaracterÃ­sticas Clave

* **âš¡ EmulaciÃ³n Acelerada:** Utiliza `PyBoy` como entorno base sin interfaz grÃ¡fica durante el entrenamiento, permitiendo velocidades superiores a **1000 FPS**.
* **ğŸ‘ï¸ ObservaciÃ³n HÃ­brida:** El agente no solo "ve", sino que "sabe" dÃ³nde estÃ¡ gracias a la inyecciÃ³n de datos hexadecimales de la RAM en la red neuronal.
* **ğŸ—ºï¸ ExploraciÃ³n Eficiente:** Sistema de recompensas densas basado en coordenadas Ãºnicas visitadas $(x, y)$ para mitigar el problema de recompensas dispersas (Sparse Rewards).
* **ğŸ¥ Streamer-Ready Architecture:** Infraestructura asimÃ©trica que permite entrenar a mÃ¡xima velocidad en segundo plano mientras se visualiza una instancia clonada a 60 FPS fluidos para transmisiÃ³n en vivo.
* **âš™ï¸ OptimizaciÃ³n de Hardware:** ImplementaciÃ³n de `SleepCallback` y gestiÃ³n de hilos (`OMP_NUM_THREADS=1`) para permitir entrenamiento y streaming simultÃ¡neo en CPUs de consumo (ej. i5/Ryzen 5) sin congelar el sistema.

## ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | TecnologÃ­a | Uso |
| :--- | :--- | :--- |
| **Lenguaje** | Python 3.11 | LÃ³gica del nÃºcleo |
| **RL Framework** | Stable-Baselines3 | ImplementaciÃ³n de PPO y VectorizaciÃ³n de Entornos |
| **Emulador** | PyBoy | Interfaz de bajo nivel con la ROM de Game Boy |
| **VisiÃ³n** | OpenCV / NumPy | Preprocesamiento de frames y renderizado |
| **Logging** | TensorBoard | Monitoreo de mÃ©tricas (Loss, Reward, Entropy) en tiempo real |

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos
* **Python 3.11** (Se recomienda usar Conda).
* **ROM de PokÃ©mon Yellow:** Debe nombrarse exactamente `PokemonYellow.gb` y colocarse en la carpeta `roms/`.

### GuÃ­a Paso a Paso

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/tu-usuario/pokemon-rl.git](https://github.com/tu-usuario/pokemon-rl.git)
    cd pokemon-rl
    ```

2.  **Crear entorno virtual:**
    ```bash
    conda create -n poke-rl python=3.11
    conda activate poke-rl
    ```

3.  **Instalar dependencias:**
    ```bash
    pip install gymnasium pyboy shimmy stable-baselines3[extra] opencv-python torch-directml
    ```

4.  **Generar Estado Inicial (Skip Intro):**
    Para evitar que el agente pierda horas de entrenamiento en el menÃº de "Nueva Partida", generamos un estado guardado justo despuÃ©s de la intro.
    ```bash
    python src/utils/create_initial_state.py
    ```
    *InstrucciÃ³n: Juega manualmente hasta tener el control del personaje en la habitaciÃ³n de Ash y cierra la ventana.*

## ğŸƒâ€â™‚ï¸ EjecuciÃ³n y Flujo de Trabajo

Este proyecto estÃ¡ diseÃ±ado para funcionar en dos terminales simultÃ¡neas: una para el "Cerebro" (Entrenamiento) y otra para los "Ojos" (Streaming).

### 1. Entrenamiento (The Brain) ğŸ§ 
Inicia el bucle de entrenamiento masivo. El sistema es "headless" (sin ventana) para maximizar velocidad.
* **Uso de CPU:** Optimizado para usar 1-2 nÃºcleos de forma intensiva.
* **Guardado:** Genera checkpoints automÃ¡ticos en `experiments/`.

```bash
python train.py

Nota: Usa Ctrl + C en cualquier momento para pausar y realizar un "Guardado de Emergencia" seguro.

2. VisualizaciÃ³n
Muestra al agente jugando en tiempo real a 60 FPS. Este script detecta automÃ¡ticamente cuando train.py guarda un nuevo modelo "mÃ¡s inteligente" y lo carga en caliente ("Hot-Reload") sin cerrar la ventana.

Bash
python watch_continuous.py

3. Monitoreo (Analytics) ğŸ“Š
Para ver grÃ¡ficas de recompensa, pÃ©rdida (loss) y entropÃ­a:

Bash
tensorboard --logdir experiments/poke_ppo_v1/logs

ğŸ§  Arquitectura del Agente
Espacio de AcciÃ³n (Action Space)
Discreto (6): [DOWN, LEFT, RIGHT, UP, A, B].

OptimizaciÃ³n: Se deshabilitaron Start y Select para reducir el ruido estocÃ¡stico y evitar que el agente se quede atascado en menÃºs.

Sistema de Recompensa (Reward Shaping)
La funciÃ³n de recompensa actual incentiva la curiosidad pura:
$$R_t = R_{exploraciÃ³n} + R_{eventos}$$
ExploraciÃ³n: +1.0 punto por cada coordenada Ãºnica $(x, y)$ visitada por mapa. Esto empuja al agente a recorrer todo el mapa disponible.
PenalizaciÃ³n de Inactividad: (ImplÃ­cita) Al no haber recompensas por quedarse quieto, el algoritmo de maximizaciÃ³n fuerza el movimiento.

ğŸ“‚ Estructura del Proyecto

pokemon-rl/
â”œâ”€â”€ config/                 # HiperparÃ¡metros y configuraciones
â”œâ”€â”€ experiments/            # Checkpoints (.zip) y Logs de TensorBoard
â”œâ”€â”€ roms/                   # Archivos del juego (.gb)
â”œâ”€â”€ states/                 # Archivos .state (Save States de PyBoy)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ pokemon_env.py  # Wrapper Gym (LÃ³gica de RAM, VisiÃ³n y Smooth Ticking)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ memory_reader.py # ExtracciÃ³n de direcciones Hex de la RAM
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.py                # Script de entrenamiento (Backend)
â”œâ”€â”€ watch_continuous.py     # Script de visualizaciÃ³n para Stream (Frontend)
â””â”€â”€ README.md               # DocumentaciÃ³n

ğŸ”® Roadmap
[ ] Implementar HippoTorch (S4) para memoria a largo plazo.

[ ] Integrar un VLM (Vision Language Model) para lectura de diÃ¡logos en pantalla.